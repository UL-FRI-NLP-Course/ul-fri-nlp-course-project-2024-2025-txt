# Natural language processing course: Analysis and comparison of translation errors and biases in LLMs

1. PURPOSE OF THE PROJECT
   
This project focuses on analyzing and comparing translation errors and biases in large language models (LLMs). We will evaluate how different models handle translations, identifying common errors such as mistranslations, omissions, and cultural misinterpretations. Additionally, the project will explore biases that emerge in translations, including gender, regional, and political biases. By systematically comparing multiple LLMs, we aim to assess translation quality using both automated metrics and human evaluations. The findings will help improve fairness and accuracy in AI-driven translation systems.

2. MEMBERS
   
   Tjaša Nadoh
   
   Urška Roblek

3. REFERENCES
   
Roberto Navigli, Simone Conia, and Björn Ross. 2023. Biases in Large Language Models: Origins, Inventory, and Discussion. J. Data and Information Quality 15, 2, Article 10 (June 2023). https://doi.org/10.1145/3597307

Barclay, P. J., & Sami, A. (2024). Investigating Markers and Drivers of Gender Bias in Machine Translations. arXiv.Org, abs/2403.11896. https://doi.org/10.48550/arxiv.2403.11896
